{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import datetime, os, yaml\n",
    "\n",
    "# Authenticating\n",
    "with open('api_key_openai', 'r') as f:\n",
    "    openai.api_key = f.read()\n",
    "    \n",
    "from langchain import OpenAI\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains.conversation.memory import (ConversationBufferMemory, \n",
    "                                                  ConversationSummaryMemory, \n",
    "                                                  ConversationBufferWindowMemory,\n",
    "                                                  ConversationKGMemory)\n",
    "from langchain.callbacks import get_openai_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRainbow Sockery'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm = OpenAI(temperature=0.1, openai_api_key=openai.api_key)\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "llm(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai.api_key)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prabhu/mambaforge/envs/fastai/lib/python3.11/site-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/prabhu/mambaforge/envs/fastai/lib/python3.11/site-packages/langchain/llms/openai.py:696: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# first initialize the large language model\n",
    "llm = OpenAI(\n",
    "\ttemperature=0,\n",
    "\topenai_api_key=openai.api_key,\n",
    "\tmodel_name=\"gpt-3.5-turbo\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", openai_api_key=openai.api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now initialize the conversation chain\n",
    "conversation = ConversationChain(llm=llm)\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Good morning AI!',\n",
       " 'history': '',\n",
       " 'response': 'Good morning to you too! How can I assist you today?'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buf = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "conversation_buf(\"Good morning AI!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'My interest here is to explore the potential of integrating Large Language Models with external knowledge',\n",
       " 'history': 'Human: Good morning AI!\\nAI: Good morning to you too! How can I assist you today?',\n",
       " 'response': \"That's a fascinating topic! Large Language Models, such as GPT-3, have shown incredible capabilities in generating human-like text. However, integrating external knowledge can help improve their accuracy and relevance in specific domains. Some approaches to integrating external knowledge include using knowledge graphs, ontologies, or pre-trained models that incorporate domain-specific knowledge. Would you like me to provide more details on any of these approaches?\"}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_buf(\"My interest here is to explore the potential of integrating Large Language Models with external knowledge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Good morning AI!\n",
      "AI: Good morning to you too! How can I assist you today?\n",
      "Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\n",
      "AI: That's a fascinating topic! Large Language Models, such as GPT-3, have shown incredible capabilities in generating human-like text. However, integrating external knowledge can help improve their accuracy and relevance in specific domains. Some approaches to integrating external knowledge include using knowledge graphs, ontologies, or pre-trained models that incorporate domain-specific knowledge. Would you like me to provide more details on any of these approaches?\n"
     ]
    }
   ],
   "source": [
    "print(conversation_buf.memory.buffer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation summary memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation summary memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/home/prabhu/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/April</td>\n",
       "      <td>April</td>\n",
       "      <td>April is the fourth month of the year in the J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/August</td>\n",
       "      <td>August</td>\n",
       "      <td>August (Aug.) is the eighth month of the year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Art</td>\n",
       "      <td>Art</td>\n",
       "      <td>Art is a creative activity that expresses imag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/A</td>\n",
       "      <td>A</td>\n",
       "      <td>A or a is the first letter of the English alph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>https://simple.wikipedia.org/wiki/Air</td>\n",
       "      <td>Air</td>\n",
       "      <td>Air refers to the Earth's atmosphere. Air is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                       url   title   \n",
       "0  1   https://simple.wikipedia.org/wiki/April   April  \\\n",
       "1  2  https://simple.wikipedia.org/wiki/August  August   \n",
       "2  6     https://simple.wikipedia.org/wiki/Art     Art   \n",
       "3  8       https://simple.wikipedia.org/wiki/A       A   \n",
       "4  9     https://simple.wikipedia.org/wiki/Air     Air   \n",
       "\n",
       "                                                text  \n",
       "0  April is the fourth month of the year in the J...  \n",
       "1  August (Aug.) is the eighth month of the year ...  \n",
       "2  Art is a creative activity that expresses imag...  \n",
       "3  A or a is the first letter of the English alph...  \n",
       "4  Air refers to the Earth's atmosphere. Air is a...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = data.to_pandas()\n",
    "w.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: ./vector_db/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1684003149221839437000\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Settings\n",
    "from chromadb.config import Settings\n",
    "client = chromadb.Client(Settings(\n",
    "    chroma_db_impl=\"duckdb+parquet\",\n",
    "    persist_directory=\"./vector_db/\" # Optional, defaults to .chromadb/ in the current directory\n",
    "))\n",
    "client.persist()\n",
    "print(client.heartbeat())\n",
    "\n",
    "# Embedding functions / Vectoriser\n",
    "from chromadb.utils import embedding_functions\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=openai.api_key,\n",
    "                model_name=\"text-embedding-ada-002\"\n",
    "            )\n",
    "\n",
    "# Collections\n",
    "collection = client.create_collection(name=\"my_collection\", embedding_function=openai_ef)\n",
    "collection = client.get_collection(name=\"my_collection\", embedding_function=openai_ef)\n",
    "\n",
    "collection.add(\n",
    "    documents=[\"lorem ipsum...\", \"doc2\", \"doc3\", ],\n",
    "    metadatas=[{\"chapter\": \"3\", \"verse\": \"16\"}, \n",
    "               {\"chapter\": \"3\", \"verse\": \"5\"}, \n",
    "               {\"chapter\": \"29\", \"verse\": \"11\"}, ],\n",
    "    ids=[\"id1\", \"id2\", \"id3\", ]\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
